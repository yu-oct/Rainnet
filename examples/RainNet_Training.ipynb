{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RainNet_Training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNnwAO9pjnf37kFVS44Grxh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PradyumnaGupta/rainnet/blob/master/RainNet_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzqFWjKfTk_u"
      },
      "source": [
        "# RainNet Training Notebook\n",
        "\n",
        "Author:[Pradyumna Gupta](https://github.com/PradyumnaGupta)\n",
        "\n",
        "This notebook is created and executed on Google Colaboratory with Google Drive Mounted."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cW2KClF0UNlO"
      },
      "source": [
        "* Mounting Google Drive\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUvMCjpZPRUl",
        "outputId": "43ee0e59-895f-47ad-bf33-4feac59bc3b6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrUscM2nUQm6"
      },
      "source": [
        "* Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVfiBtxzdlF7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c3656d8-0046-4654-e5ac-76c39228bfc5"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt \n",
        "import imageio\n",
        "import PIL\n",
        "from PIL import ImageFile\n",
        "import cv2\n",
        "import numpy as np\n",
        "from IPython.display import display\n",
        "from tqdm import tqdm\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "# import pretrainedmodels\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import f1_score,accuracy_score,roc_auc_score\n",
        "import math\n",
        "import time\n",
        "import albumentations\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ai_0aJr1UUcO"
      },
      "source": [
        "* Defining some data preparing and preprocessing functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcUSl1wfdeOt"
      },
      "source": [
        "def Scaler(array):\n",
        "    return np.log(array+0.01)\n",
        "\n",
        "\n",
        "def invScaler(array):\n",
        "    return np.exp(array) - 0.01\n",
        "\n",
        "\n",
        "def pad_to_shape(array, from_shape=900, to_shape=928, how=\"mirror\"):\n",
        "    # calculate how much to pad in respect with native resolution\n",
        "    padding = int( (to_shape - from_shape) / 2)\n",
        "    # for input shape as (batch, W, H, channels)\n",
        "    if how == \"zero\":\n",
        "        array_padded = np.pad(array, ((0,0),(padding,padding),(padding,padding),(0,0)), mode=\"constant\", constant_values=0)\n",
        "    elif how == \"mirror\":\n",
        "        array_padded = np.pad(array, ((0,0),(padding,padding),(padding,padding),(0,0)), mode=\"reflect\")\n",
        "    return array_padded\n",
        "\n",
        "\n",
        "def pred_to_rad(pred, from_shape=928, to_shape=900):\n",
        "    # pred shape 12,928,928\n",
        "    padding = int( (from_shape - to_shape) / 2)\n",
        "    return pred[::, padding:padding+to_shape, padding:padding+to_shape].copy()\n",
        "\n",
        "\n",
        "def data_preprocessing(X):\n",
        "    \n",
        "    # 0. Right shape for batch\n",
        "    X = np.moveaxis(X, 0, -1)\n",
        "    X = X[np.newaxis, ::, ::, ::]\n",
        "    # 1. To log scale\n",
        "    X = Scaler(X)\n",
        "    # 2. from 900x900 to 928x928\n",
        "    X = pad_to_shape(X)\n",
        "    \n",
        "    return X\n",
        "\n",
        "\n",
        "def data_postprocessing(nwcst):\n",
        "\n",
        "    # 0. Squeeze empty dimensions\n",
        "    nwcst = np.squeeze(np.array(nwcst))\n",
        "    # 1. Convert back to rainfall depth\n",
        "    nwcst = invScaler(nwcst)\n",
        "    # 2. Convert from 928x928 back to 900x900\n",
        "    nwcst = pred_to_rad(nwcst)\n",
        "    # 3. Return only positive values\n",
        "    nwcst = np.where(nwcst>0, nwcst, 0)\n",
        "    return nwcst"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqUqGtlrUiAf"
      },
      "source": [
        "* Constructing Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3slT2sUndeMp"
      },
      "source": [
        "class Dataset(tf.keras.utils.Sequence):\n",
        "    \n",
        "    def __init__(\n",
        "            self, \n",
        "            dataset_dict,\n",
        "            image_names,\n",
        "            batch_size\n",
        "    ):\n",
        "        self.keys = image_names\n",
        "        self.dataset = dataset_dict\n",
        "        self.bs = batch_size\n",
        "\n",
        "    def get_index(self,i):\n",
        "      x = []\n",
        "      for j in range(4):\n",
        "        try:\n",
        "          arr = np.array(self.dataset.get(self.keys[i+j]))\n",
        "        except:\n",
        "          print(i,j)\n",
        "        x.append(arr)\n",
        "      \n",
        "      x = data_preprocessing(np.stack(x,0))\n",
        "      # x = np.transpose(np.squeeze(x),(2,0,1)) \n",
        "      x = np.squeeze(x)\n",
        "      y = np.squeeze(data_preprocessing(np.array(self.dataset[self.keys[i+3]])[np.newaxis,:,:]))\n",
        "\n",
        "      return x.astype('float32'),y.astype('float32')\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        \n",
        "      X = []\n",
        "      Y = []\n",
        "\n",
        "      for i in range(index*self.bs,(index+1)*self.bs):\n",
        "        x,y = self.get_index(i)\n",
        "        X.append(x[np.newaxis,:])\n",
        "        Y.append(y[np.newaxis,:])\n",
        "\n",
        "      return X,Y\n",
        "        \n",
        "    def __len__(self):\n",
        "      return (len(self.keys) - 4)//self.bs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvyHVgK3UprR"
      },
      "source": [
        "* Loading the dataset dictionary.\n",
        "\n",
        "The dataset consists of a large dictionary where the keys are the image names and the value is the actual image in the form of a numpy array.The dataset can be downloaded from [here](https://drive.google.com/file/d/1sZI4TbFkgJcpkZDBfbuWR_JBZSNhKow4/view?usp=sharing)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBMTBcp5deHx"
      },
      "source": [
        "import h5py\n",
        "dataset_dict = h5py.File('drive/MyDrive/RYDL.hdf5', 'r')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMvLHuAAVqEk"
      },
      "source": [
        "* Loading the image names.\n",
        "\n",
        "The image names are present in the keys txt file which can be downloaded from [here](https://drive.google.com/file/d/1DvVUyrUvL4P8TRr_y_r5NrOxlRjwgWQr/view?usp=sharing). \n",
        "\n",
        "The data from the year 2012-2016 will be used for training purposes and the data from the year 2017 will be used for validation purposes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEmMAg5xdeFW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acd7f88f-b13f-4557-8d2e-a9c66a0cd559"
      },
      "source": [
        "import ast\n",
        "with open('drive/MyDrive/RYDL_keys.txt','r') as f:\n",
        "  image_names = ast.literal_eval(f.read())\n",
        "image_names = [name for name in image_names if name[:4]>'2012']\n",
        "\n",
        "train_images = [name for name in tqdm(image_names) if \"2017\" not in name]\n",
        "val_images = [name for name in tqdm(image_names) if name[0:4]==\"2017\"]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 221211/221211 [00:00<00:00, 2273532.11it/s]\n",
            "100%|██████████| 221211/221211 [00:00<00:00, 1789822.69it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrYW92QDWOll"
      },
      "source": [
        "* Instantiating the dataloaders."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvIs5J1Mknig"
      },
      "source": [
        "train_dataset = Dataset(\n",
        "    dataset_dict=dataset_dict,\n",
        "    image_names=train_images,\n",
        "    batch_size=1\n",
        ")\n",
        "\n",
        "valid_dataset = Dataset(\n",
        "    dataset_dict=dataset_dict,\n",
        "    image_names=val_images,\n",
        "    batch_size=1\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8dGA3a-WUPz"
      },
      "source": [
        "* Constructing the RainNet model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mA8ePVeaknfq"
      },
      "source": [
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "\n",
        "def rainnet(input_shape=(928, 928, 4), mode=\"regression\"):\n",
        "\n",
        "    \"\"\"\n",
        "    The function for building the RainNet (v1.0) model from scratch\n",
        "    using Keras functional API.\n",
        "    Parameters:\n",
        "    input size: tuple(W x H x C), where W (width) and H (height)\n",
        "    describe spatial dimensions of input data (e.g., 928x928 for RY data);\n",
        "    and C (channels) describes temporal (depth) dimension of \n",
        "    input data (e.g., 4 means accounting four latest radar scans at time\n",
        "    t-15, t-10, t-5 minutes, and t)\n",
        "    \n",
        "    mode: \"regression\" (default) or \"segmentation\". \n",
        "    For \"regression\" mode the last activation function is linear, \n",
        "    while for \"segmentation\" it is sigmoid.\n",
        "    To train RainNet to predict continuous precipitation intensities use \n",
        "    \"regression\" mode. \n",
        "    RainNet could be trained to predict the exceedance of specific intensity \n",
        "    thresholds. For that purpose, use \"segmentation\" mode.\n",
        "    \"\"\"\n",
        "\n",
        "    inputs = Input(input_shape)\n",
        "\n",
        "    conv1f = Conv2D(64, 3, padding='same', kernel_initializer='he_normal')(inputs)\n",
        "    conv1f = Activation(\"relu\")(conv1f)\n",
        "    conv1s = Conv2D(64, 3, padding='same', kernel_initializer='he_normal')(conv1f)\n",
        "    conv1s = Activation(\"relu\")(conv1s)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1s)\n",
        "\n",
        "    conv2f = Conv2D(128, 3, padding='same', kernel_initializer='he_normal')(pool1)\n",
        "    conv2f = Activation(\"relu\")(conv2f)\n",
        "    conv2s = Conv2D(128, 3, padding='same', kernel_initializer='he_normal')(conv2f)\n",
        "    conv2s = Activation(\"relu\")(conv2s)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2s)\n",
        "\n",
        "    conv3f = Conv2D(256, 3, padding='same', kernel_initializer='he_normal')(pool2)\n",
        "    conv3f = Activation(\"relu\")(conv3f)\n",
        "    conv3s = Conv2D(256, 3, padding='same', kernel_initializer='he_normal')(conv3f)\n",
        "    conv3s = Activation(\"relu\")(conv3s)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3s)\n",
        "\n",
        "    conv4f = Conv2D(512, 3, padding='same', kernel_initializer='he_normal')(pool3)\n",
        "    conv4f = Activation(\"relu\")(conv4f)\n",
        "    conv4s = Conv2D(512, 3, padding='same', kernel_initializer='he_normal')(conv4f)\n",
        "    conv4s = Activation(\"relu\")(conv4s)\n",
        "    drop4 = Dropout(0.5)(conv4s)\n",
        "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
        "\n",
        "    conv5f = Conv2D(1024, 3, padding='same', kernel_initializer='he_normal')(pool4)\n",
        "    conv5f = Activation(\"relu\")(conv5f)\n",
        "    conv5s = Conv2D(1024, 3, padding='same', kernel_initializer='he_normal')(conv5f)\n",
        "    conv5s = Activation(\"relu\")(conv5s)\n",
        "    drop5 = Dropout(0.5)(conv5s)\n",
        "\n",
        "    up6 = concatenate([UpSampling2D(size=(2, 2))(drop5), conv4s], axis=3)\n",
        "    conv6 = Conv2D(512, 3, padding='same', kernel_initializer='he_normal')(up6)\n",
        "    conv6 = Activation(\"relu\")(conv6)\n",
        "    conv6 = Conv2D(512, 3, padding='same', kernel_initializer='he_normal')(conv6)\n",
        "    conv6 = Activation(\"relu\")(conv6)\n",
        "\n",
        "    up7 = concatenate([UpSampling2D(size=(2, 2))(conv6), conv3s], axis=3)\n",
        "    conv7 = Conv2D(256, 3, padding='same', kernel_initializer='he_normal')(up7)\n",
        "    conv7 = Activation(\"relu\")(conv7)\n",
        "    conv7 = Conv2D(256, 3, padding='same', kernel_initializer='he_normal')(conv7)\n",
        "    conv7 = Activation(\"relu\")(conv7)\n",
        "\n",
        "    up8 = concatenate([UpSampling2D(size=(2, 2))(conv7), conv2s], axis=3)\n",
        "    conv8 = Conv2D(128, 3, padding='same', kernel_initializer='he_normal')(up8)\n",
        "    conv8 = Activation(\"relu\")(conv8)\n",
        "    conv8 = Conv2D(128, 3, padding='same', kernel_initializer='he_normal')(conv8)\n",
        "    conv8 = Activation(\"relu\")(conv8)\n",
        "\n",
        "    up9 = concatenate([UpSampling2D(size=(2, 2))(conv8), conv1s], axis=3)\n",
        "    conv9 = Conv2D(64, 3, padding='same', kernel_initializer='he_normal')(up9)\n",
        "    conv9 = Activation(\"relu\")(conv9)\n",
        "    conv9 = Conv2D(64, 3, padding='same', kernel_initializer='he_normal')(conv9)\n",
        "    conv9 = Activation(\"relu\")(conv9)\n",
        "    conv9 = Conv2D(2, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv9)\n",
        "    \n",
        "    if mode == \"regression\":\n",
        "        outputs = Conv2D(1, 1, activation='linear')(conv9)\n",
        "    elif mode == \"segmentation\":\n",
        "        outputs = Conv2D(1, 1, activation='sigmoid')(conv9)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Rd7DR-tWcKi"
      },
      "source": [
        "* Instantiating and compiling the model with Adam optimizer and Log_Cosh loss function.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chgdwXYGbx1i"
      },
      "source": [
        "model = rainnet()\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(lr=3e-4),loss='log_cosh')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qu0tg8XbWqao"
      },
      "source": [
        "* Start Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6Xe3fDrwRYs"
      },
      "source": [
        "model.fit(x=train_dataset,validation_data=valid_dataset,epochs=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYAip10WIBcd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}